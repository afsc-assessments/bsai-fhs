---
title: "GOA FHS Assessment Bridging (version & data)"
author: "M Sosa Kapur maia.kapur@noaa.gov"
output:
  html_notebook:
    theme: united
    toc: yes
---
# Bridging from SS3v3.24U to SSv3.30.17
C. Monnahan began this effort in summer of 2021. He found that the `ss_trans.exe` software was unable to work on the 2017 model as-was due to the following line in the .dat file:
`-1 0 #_surveytiming_in_season`.  R. Methot suggested converting this line to `-1.5  #_surveytiming_in_season`, which enabled the transition function to complete,
but modified the timing of the survey, thus producing discrepant likelihoods. R. Methot then suggested changing the survey index month to 1.0 (in the new v3.30 dat file), but added that v3.24 could only produce expected length-composition values using midseason age-length keys. In other words, the previous version of this model was estimating length composition values assuming that the age-length key applied to the middle of the year, so there will necessarily be a discrepancy between the structure and magnitude of vulnerable biomass to the survey if those data are specified as being observed at the beginning of the year.

To work around this, Rick indicated we could instead adjust only the survey composition month ("seas") to 7.0, essentially aligning the compositional observations with the mid-year biomass. We acknowledge this is a kludge considering the compositions and indices come from the same surveys; for future benchmark updates, scientists may elect to change the survey data season universally (in line with the model labeled `old-MLEs` below). This cycle, we elected to use the approach which produced results most similar to the 2017 benchmark.

The following documents the steps taken to explore this issue and arrive at a bridged baseline model in SS3v3.30.17. The likelihoods and derived quantities are similar (and indistinguishable for management purposes) but we do not believe values will ever be an exact match due to differences in how survey timing is handled between versions.
```{r, include = FALSE, warning = FALSE, message = FALSE}
require(r4ss)
require(here)
require(tidyverse)

models <- c('3.24U','3.30_old_mles', '3.30_adjust_timing', '3.30_converted')
colrs <- c('black',alpha('goldenrod',0.2),alpha('dodgerblue2',0.2),'seagreen4')
outlist <- lapply(paste0(here('model_runs','01_bridging','cole',models)),
                  function(x) c(model=x, SS_output(x, covar=FALSE, printstats=FALSE, verbose=FALSE)))
 
group1 <- SSsummarize(list(outlist[[1]],outlist[[2]],outlist[[3]]))
group2 <- SSsummarize(list(outlist[[1]],outlist[[2]],outlist[[3]],outlist[[4]])) 

## load 2017 model and fully converted model separately
m0_0 <- SS_output(here('model_runs','m0_0'), covar=TRUE, verbose=FALSE, printstats=FALSE) 
m2017 <- SS_output(here('model_runs','2017-mod'), covar=TRUE, verbose=FALSE, printstats=FALSE) 
```


## Transition with updated survey timing
### Survey index, length and CAAL month = 7.0 
Take the `.ss_new` files from model 3.24U, update the survey index **and** composition timing to 7.0, and run `ss_trans.exe v3.30.17` using command `-maxI -1`. This command overrides the max phase set in the starter file (effectively disabling estimation).

## Pass old MLEs for rec devs
### Survey index, length and CAAL month = 7.0 
This model takes the `.ss_new` files from the transitioned model, and pastes in the MLEs for recruitment devs from the original model (3.24) into the `.par` file (both recdev early and main). This exercise demonstrates that the population dynamics are the same when the forecasted recruitment devs from the original model (3.24U) are passed into the `.ss_new` files from the transitioned model.
 
## Adjust timing
### Survey index month = 1.0, length and CAAL month = 7.0 
Using the `.ss_new` files from the previous model (with `3.24U`'s rec devs), turn of the estimation of forecasted recruitment devs (`forecast dev phase = -1` in starter), **change the survey index timing back to month = 1.0** and re-run using the original MLEs (`ss.par`, no estimation). This is effectively re-introducing the survey timing as specified in the original model while keeping all the optimized the parameters the same; the fit to the index does improve, but it is not an exact match. **This demonstrates that the discrepancies between versions is caused by the way survey timing is handled in the internal dynamics of v3.30+.**
 

## Optimize (full conversion)
This is the same as the previous model, but with optimization enabled. **This model ("converted", green line below) acts as the baseline transitioned model for 3.30.17.** After optimization, the index matches well, but it scales the population up ($R_0$ increases from 12.8219 to 12.8948).  

```{r, echo = F, warning = FALSE, message = FALSE}

SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=1)

```

```{r, echo = F, warning = FALSE, message = FALSE}
SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=13)
```
Note that the vulnerable biomass (to the survey) has changed between versions, due to timing.
```{r, echo = F, warning = FALSE, message = FALSE}
SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=1)
SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=16)
bio <- lapply(outlist, function(x)
  cbind(model=x$model, x$timeseries[,c('Yr','SpawnBio','Bio_all')])) %>% bind_rows
indices <- lapply(outlist, function(x)
  cbind(model=x$model, x$cpue[,c('Yr','Vuln_bio')])) %>% bind_rows

p1 <- ggplot(bio, aes(Yr, SpawnBio, color=model)) + 
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position =  'none')+
  scale_color_manual(values = colrs, labels = models)

p2 <- ggplot(bio, aes(Yr, Bio_all, color=model))+
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position = 'none')+
  scale_color_manual(values = colrs, labels = models)

p3 <- ggplot(indices, aes(Yr, Vuln_bio, color=model))  +
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position = 'bottom')+
  scale_color_manual(values = colrs, labels = models)

require(patchwork) 
(p1 | p2) /p3
```

Tabular comparison of index estimates across models.
```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
indices %>% mutate(model = basename(model)) %>% pivot_wider(c('Yr'), names_from='model', values_from='Vuln_bio')
```

Tabular comparison of likelihoods across models.
```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
likes <- lapply(outlist, function(x)
  data.frame(component=row.names(x$likelihoods_used), 
             model=basename(x$model), x$likelihoods_used)) %>%
  bind_rows %>% remove_rownames %>% arrange(component, model) %>% filter(abs(values)>.01)
likes%>% pivot_wider(c('component'), names_from='model', values_from='values') %>%
  select(component, '3.24U','3.30_old_mles','3.30_adjust_timing','3.30_converted' )
```
Tabular comparison of model scale (R0 estimates) across models.
```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
r0 <- lapply(outlist, function(x)
  cbind(model=basename(x$model), x$parameters[,c('Label','Value')])) %>%
  bind_rows %>% remove_rownames %>% filter(Label %in% c("SR_LN(R0)"))
r0 %>% pivot_wider(c('Label'), names_from='model', values_from='Value')
```


 
## Attempt survey index timing month  = 7
### Survey index month = 7.0, length and CAAL month = 7.0 

Out of curiosity, we investigated the impact of (correctly) specifying the survey index month as 7.0 and disabling optimization, using the parameter values from `3.30_converted` above. Total SSB is distinct among the three (with the updated timing model even more different), though the survey fits are indistinguishable.
```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
models <- c('3.24U','m02_3.30_converted','m03_update_timing','m04_update_ageing_error','m05_VAST_index')
## m01 = v3.24U and m02 = 3.30_converted above, just loaded individually now
m01  <- SS_output(here('model_runs','01_bridging','cole','m01_2017_3.24U'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m02 <- SS_output(here('model_runs','01_bridging','cole','m02_2017_3.30.17'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m03 <- SS_output(here('model_runs','01_bridging','cole','m03_update_timing'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m04 <- SS_output(here('model_runs','01_bridging','cole','m04_update_ageing_error'), covar=TRUE, verbose=FALSE, printstats=FALSE)

SSplotComparisons(SSsummarize(list(m01,m02,m03)),  uncertainty=TRUE, subplot = 1, legendlabels = models)
SSplotComparisons(SSsummarize(list(m01,m02,m03)),  uncertainty=TRUE, subplot = 13, legendlabels = models)
```


## VAST data 
I received the model-based index with data through 2021 from GAP in Spring 2022. Since I plan to present this as a sensitivity to the proposed base model, we should also illustrate how the use of VAST would have affected the previous [converted] benchmark. This model is identical to `m02_3.30_converted` otherwise (the survey index timing is 1.0). As we might expect, the SSB is higher in the VAST version since the model-based inputs are higher than the design-based since ~2000. Reassuringly the dynamics are largely similar between converted models for the years where the VAST & design-based inputs are more alike.


```{r echo=FALSE, out.width = '40%'}
knitr::include_graphics(here('figs','2022-04-18-index_VASTvsDesign.png'))
```


```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
m05 <- SS_output(here('model_runs','01_bridging','cole','m05_VAST_index'), covar=TRUE, verbose=FALSE, printstats=FALSE)

```

```{r echo=FALSE, out.width = '40%'}
SSplotComparisons(SSsummarize(list(m01,m02,m05)),  uncertainty=TRUE, subplot = 1, legendlabels = models[c(1,2,5)])
SSplotComparisons(SSsummarize(list(m01,m02,m05)),  uncertainty=TRUE, subplot = 13, legendlabels = models[c(1,2,5)])
```


# New aging error matrix
The SSC recommended exploration/addition of a new aging error matrix. To that end, we have a few different options, and it is desirable to illustrate how they affect the base model (no new data) before proceeding.

The present GOA aging error matrix actually comes from the BSAI assessment, and basically says that error increases linearly to a maximum at age 16. We do have read-replicate data for GOA FHS. We created several candidate models that either treated the individual readers separately (there are ~10 of them) or pooled the data (meaning the difference between a read by Delsa & Jon is equivalent to the difference between Tom & Mary). The best AIC was obtained by a pooled-data model which assumed constant bias and sigma across readers (bias is the different-integer age read, and sigma is the variation in true age). This is called `aic2p` in the `ageing_error/real_data` folder. The bias is otherwise identical between approaches.

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
newerr <- t(read.csv(here("data","ageing_error", "real_data", "aic2p", "ss_format_reader 1.csv"))) %>%
data.frame()
names(newerr) <- newerr[1,]
newerr <- newerr[-1,]

with(m0_0$age_error_sd, plot( type1 ~ age, pch = 19, col =  'grey22', ylim = c(0,3)))
with(newerr, points( SD ~True_Age, pch = 19, col =  'blue', add = T))
legend('topleft', 
       legend = c('old matrix (BSAI)', 'new matrix (AIC)'),
       col = c('grey22','blue'), pch = 19)
knitr::include_graphics(here("data", "ageing_error","real_data","aic2p","compplots.png"))

```
Here I run the converted model with the new aging error matrix from `aic2p`. This involved replacing the chunk in the `.dat` file with the `1 age err definition` and a line for the bias ("expected age" from the package output) and CV. There is not a strong change in derived quantities.
```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c('3.24U (2017)','3.30_converted','3.30_newAgeErr')
m0_4 <- SS_output(here('model_runs','01_bridging','cole','m04_update_ageing_error'))
# SS_plots(m0_4)
SSplotComparisons(SSsummarize(list(m2017,m0_0,m0_4)),  uncertainty=TRUE, subplot = 1, legendlabels = models)


knitr::include_graphics(here('model_runs','01_bridging','cole','m04_update_ageing_error',
                             "plots",
                             "comp_condAALfit_Andre_plotsflt2mkt0_page2.png"))
knitr::include_graphics(here('model_runs','01_bridging','cole','m04_update_ageing_error',
                             "plots",
                             "comp_condAALfit_Andre_plotsflt2mkt0_page3.png"))
```

# Data Bridging 
I copied `m02_2017_3.30.17` into the main `model_runs` dir and renamed it to "m0_0".
Here I start with the transitioned model and sequentially add data. Unless otherwise specified, the entire data series was replaced with the new extraction to accomodate for any changes in our databases. There are no changes made to the parameter specifications (priors, start values, etc).
The following changes were also made to peripheral SS3 files before beginning; these apply to models `m0_1` onwards:

+ In `control.ss`, the main period of recruitment devs apparently hasn't changed from 2012 upon the last assessment. For consistency I will leave that as-is for now, but likely will adjust as a sensitivity. However, I did change `_end_yr_for_ramp_in_MPD` to 2026.4. Time blocks do not cross the endyr so I made no adjustments there.

+ In `data.ss` change `endyr` to 2022.

+ Ensure estimation is turned on in `starter.ss`

+ In `forecast.ss` first year for caps and allocations = 2023, update rebuilder years, though **note** we do not use the forecast file whatsoever in this assessment.

+ Functionality check was conducted in dir `/m0_0check` to confirm that time series estimation is pushed forward as expected.

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c('m0_0','m0_0check')

m0_0check <- SS_output(here('model_runs','m0_0check'), covar=F, verbose=FALSE, printstats=FALSE)
SSplotComparisons(SSsummarize(list(m0_0,m0_0check)),  uncertainty=TRUE, subplot = 1, legendlabels = models)
SSplotComparisons(SSsummarize(list(m0_0,m0_0check)),  uncertainty=TRUE, subplot = 11, legendlabels = models)
```

## Add catch `m0_1`
Right now 2022 catches are a placeholder, will project closer to the time of.
```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c('2017 Base','+catch', '+survey', '+fish_len','+goa_len','+CAAL')
m0_1 <- SS_output(here('model_runs','m0_1'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_1,    png = T, dir = here('model_runs','m0_1'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1 )),  plot = FALSE, png = T, plotdir = here('model_runs','m0_1'), uncertainty=TRUE,  legendlabels = models)
knitr::include_graphics(here('model_runs','m0_1','plots','data_plot2.png'))
```
## Add design-based survey index `m0_2`
In addition to the 2019 & 2021 datapoints, this time series has a different value for 2001, about 18mt less than previous. Discussion with RACE and review of both the AFSC database and previous SAFE tables indicated we should go with this value -- it is not clear where the ~170mt value came from previously.
```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
m0_2 <- SS_output(here('model_runs','m0_2'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_2, png = T, dir = here('model_runs','m0_2'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_2'), uncertainty=TRUE,  legendlabels = models)
```

## Add fishery length comps `m_03`

As previously, data before 1989 are included but turned off. I noticed that the input data in the previous model is in somewhat strange raw numbers (i.e an observed number of 132.708 within a sample size of 46). When you run `ss_output()` on the original data, it returns `lendbase` scaled to 1, which I used to spot check that the extracted data was accurate in the `00_getData.R` script. So, I replaced the entire data frame in this step, acknowledging that the input scale is different.

The aggregate fit is OK but the residuals in the early period are awful (this was the case before as well).

The following years of the fishery length comps are ghosted: `r paste(unique(m0_3$ghostlendbase$Yr), collapse = ", ")` 
```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
m0_3 <- SS_output(here('model_runs','m0_3'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_3,   png = T, dir =   here('model_runs','m0_3'))
#   SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2,m0_3)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_3'), uncertainty=TRUE,  legendlabels = models) 
```

```{r, echo=FALSE, out.height = '25%'}
knitr::include_graphics(here('model_runs','m0_3','plots','comp_lenfit__aggregated_across_time.png'))
knitr::include_graphics(here('model_runs','m0_3','plots','comp_lenfit_residsflt1mkt0_page2.png'))
```

## Add survey length comps `m_04`
The nsamp (nhauls) has changed by 1 or so across years between the old version and this. I updated the data-prep method from Carey's code into `tidyverse` and spot checked that the values were consistent among entries (with a rounding tolerance of 1e-4). 
```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
m0_4 <- SS_output(here('model_runs','m0_4'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_4,   png = T, dir =   here('model_runs','m0_4'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2,m0_3,m0_4)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_4'), uncertainty=TRUE,  legendlabels = models)
```



## Add survey CAALs & marginal survey ages as ghost fleet `m0_5`
Here I incorporate the conditional ages at length as well as the marginal ages, both from the GOA survey thru year 2019; the latter are read in as a ghost fleet and don't contribute to the likelihoods. **Note the marginal ghost fleet is still missing values from 2017 & 2019. Need to track down `AGEPOP` variable (which is in RACEBASE but not in provided csvs) to calculate freqs for those years**.

Also note that this model has not yet dealt with the various `warning` messages, and has retained the Francis weighting values from the benchmark. Notably, those values downweight the age comp data to 0.34.


```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
m0_5 <- SS_output(here('model_runs','m0_5'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_5,   png = T, dir =   here('model_runs','m0_5'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2,m0_3,m0_4,m0_5)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_5'), uncertainty=TRUE,  legendlabels = models)
```

# Sanity Checks and Explorations

Now that all the data are added let's do some sanity checks. It's evident that the survey data made the biggest change in the SSB trend, and did so in a logical direction. The 2017 base model erroneously uses a value of ~170k mt in the 2001 survey, which has since been revised/corrected down to about 150k mt.

```{r, echo=FALSE, out.width = '25%'}
knitr::include_graphics(here('model_runs','m0_5','compare2_spawnbio_uncertainty.png'))
```
## What survey data causes downshift? `m0_5b:d`

Here are some tests runs with the survey data:

+ `m0_5b`: this is `m0_5` **year 2001** reverted to the ~170,000k value which was used previously in year 2001, to see if downshifting this year bends the trend downards;
+ `m0_5c`: this is `m0_5` with  survey data truncated back to 2017 (to see if the later years alone are causing the downshift, which would be sensible considering the trend), and
+ `m0_5d`: this is `m0_5` with both the changes mentioned above (revert year 2001 and remove values after 2017). This is generally equivalent to using the 2017 benchmark survey data since all values besides 2001 were consistent between design-based survey pulls.


```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c("2017 base", "m0_5", "m0_5b [reverted 2001 survey]", 
           "m0_5c [truncate survey]", "m0_5d [revert & truncate]")
m0_5b <- SS_output(here('model_runs','m0_5b'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m0_5c <- SS_output(here('model_runs','m0_5c'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m0_5d <- SS_output(here('model_runs','m0_5d'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SSplotComparisons(SSsummarize(list(m0_0,m0_5,m0_5b,m0_5c,m0_5d)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_5d'), uncertainty=TRUE,  legendlabels = models)
```

```{r  echo = F, warning = FALSE, message = FALSE, out.width='25%'}
knitr::include_graphics(here('model_runs','m0_5d','compare2_spawnbio_uncertainty.png'))
knitr::include_graphics(here('model_runs','m0_5d','compare13_indices.png'))
```
This exercise demonstrates that a) the ~20 mt difference in the 2001 survey is not driving the changes between models, as `m0_5b` below is still downshifted, and b) truncation alone is enough to get the trend back in line with the 2017 base model. Thus I do not plan to change individual years of survey data.

*However*, the fits to those down years is quite poor.I notice that fits to CAALs and lengths are quite good especially in recent years, but the CAAL sd diagnostic plots are *rough and worsening* for later years; my hope is this can get addressed via the aging error updates, and possibly by extending the main rec devs period.

Specifically, the observed CAALs are decently stable and well fit in recent years, but the SDs are huge and missed basically throughout (these plots are from `m0_5`), pointing to an aging error issue. Since the expected SDs are uniformly higher, it suggests that the model things there is more variance in age at length than what we've provided. Coupled with the fact that we have rec-devs disabled since 2012 (when the main period ends), we aren't allowing for the decreased observed index to be reflected in reduced recruitment output.

```{r  echo = F, warning = FALSE, message = FALSE, out.width='25%'}
knitr::include_graphics(here('model_runs','m0_5','plots','comp_condAALfit_Andre_plotsflt2mkt0_page3.png' ))
knitr::include_graphics(here('model_runs','m0_5','plots','comp_condAALfit_Andre_plotsflt2mkt0_page4.png' ))
```

## What happens when the main rec-dev period gets pushed out? `m0_6`

This model takes `m0_5` and extends the main rec-dev period to 2021 in the control file. Line 127 was shifted from 2012 to 2021, and line 136 (last year of full bias adjustment) shifted to 2019.1. This allows the model to take information from the next ~10 years of surveys/comps into account when calculating rec devs.

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c("2017 base", "m0_5", "m0_6 [rec dev shift]")
m0_6 <- SS_output(here('model_runs','m0_6'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_6,   png = T, dir =   here('model_runs','m0_6'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_5,m0_6)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_6'), uncertainty=TRUE,  legendlabels = models)
```
Updating the rec-devs does not seem to cause better fits to the survey, nor does it indicate low recruitment in the years leading up to the low survey observations.While I think it makes sense to extend the main rec-dev period, this is clearly not enough to grant the flexibility required to better fit the recent years of survey data.
```{r  echo = F, warning = FALSE, message = FALSE, out.width='25%'}
knitr::include_graphics(here('model_runs','m0_6','compare2_spawnbio_uncertainty.png'))
knitr::include_graphics(here('model_runs','m0_6','compare11_recdevs.png'))
knitr::include_graphics(here('model_runs','m0_6','compare13_indices.png'))
```

## Can the model as-is better fit the VAST data? `m0_7`

This takes model `m0_5` (retains the rec-dev main period ending in 2012) and replaces the series with the model-based VAST data, which has a stabler, higher trend in recent years. Note the points on the comparison plot below are likely the design based inputs.

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c("2017 base", "m0_5", "m0_7 [VAST data instead]")
m0_7 <- SS_output(here('model_runs','m0_6'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_7,   png = T, dir =   here('model_runs','m0_7'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_5,m0_7)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_7'), uncertainty=TRUE,  legendlabels = models)
```


The model seems largely insensitive to the survey data, at least at the scale difference we're seeing here. The stability and upward trend in recent years is conserved at the expense of poor fits to the index. That said, there really is not a ton of information in our index, which means the model is likely relying more heavily on other information sources (namely lengths).




```{r  echo = F, warning = FALSE, message = FALSE, out.width='25%'}
knitr::include_graphics(here('model_runs','m0_7','compare2_spawnbio_uncertainty.png'))
knitr::include_graphics(here('model_runs','m0_7','compare11_recdevs.png'))
knitr::include_graphics(here('model_runs','m0_7','compare13_indices.png'))
```


# Fine-scale adjustments to `m0_5`

This section aims to 1) eliminate `warning` messages, as appropriate, 2) explore removing some of the benchmark settings (namely the tuning steps), and 3) update specifications.


