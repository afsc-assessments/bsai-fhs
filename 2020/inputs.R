#### Process the raw data to make assessment data inputs and
#### plots of data

### Inputs: Raw data files as generated by data.R

### Outputs: Processed data inputs in flatfiles, an SS .dat file,
### plus plots exploring and checking the data

### -----------------------
## Libraries/options required:
library(tidyverse)
theme_set(theme_bw())
library(r4ss)
packageVersion('r4ss') # 1.40.0
### This contains a set of functions I got from Carey originally
### from Steve that do data processing.
source("../../colesbss/functions.R")
## The previous .dat file to be modified or used for comparing
SS_dat <- SS_readdat('inputs/2018_BSAI_FHS_1.dat', verbose=FALSE)
max_age <- 21
age_bins <- seq(1,max_age,by = 1)
bin_width <- 2
## length bins to use for fsh and srv length comp data
max_size <- 40 ##65 = rex ##70 = Dover and Flathead
min_size <- 6
bin_width <- 2
len_bins <- c(seq(min_size,max_size,bin_width),43,46,49,52,55,58)
## for CAAL length bins (does not match!)
caal_len_bins <- seq(6, 58, by=2)
options(digits=7)


### ----- Catch from AKFIN -----
message("Processing catch data...")
catch <- read.csv('data/catch.csv')
## Type= (R)etained or (D)iscarded
g <- catch %>% group_by(YEAR, ZONE, TYPE, GEAR) %>%
  summarize(TONS=sum(TONS), .groups='drop') %>%
  ggplot(aes(YEAR, TONS, color=GEAR)) +
  geom_line() + facet_grid(ZONE~TYPE, scales='free') + scale_y_log10()
ggsave('inputs/plots/catch_by_gear_zone.png', g, width=7, height=5)
## Aggregate catch to just year
annual_catch <- catch %>%
  group_by(YEAR) %>% summarize(total=sum(TONS), .groups='drop') %>% data.frame()
## Build SS structure
SS_catch <- data.frame(year=annual_catch$YEAR, seas=1, fleet=1,
           catch=annual_catch$total, catch_se=.01)
write.csv(x=SS_catch, file='inputs/SS_catch.csv', row.names=FALSE)


### ----- Get survey biomass estimates -----
message("Processing biomass data...")
index_ebs <-
  read.csv("data/biomass_survey_ebs.csv") %>%
  select(year=YEAR, biomass=BIOMASS,
         variance=VARBIO) %>% cbind(survey='ebs')
index_ai <- read.csv('data/biomass_survey_ai.csv') %>%
  mutate(species=gsub(" ", "_",COMMON_NAME)) %>%
  select(year=YEAR, biomass=TOTAL_BIOMASS,
         variance=BIOMASS_VAR) %>% cbind(survey='AI')
index_raw <- rbind(index_ebs, index_ai) %>%
  pivot_wider(names_from=survey, values_from=c(biomass, variance))
## Do a linear regression to get missing AI years
index_raw  <- index_raw %>% mutate(sd_ebs=sqrt(variance_ebs), sd_AI=sqrt(variance_AI))
z1 <- subset(index_raw, !is.na(biomass_AI))
z2 <- subset(index_raw, is.na(biomass_AI))
lmbio <- lm(biomass_AI~biomass_ebs, data=z1)
z2$biomass_AI <- as.numeric(predict(lmbio, newdata=z2))
lmvar <- lm(sd_AI~sd_ebs, data=z1)
z2$sd_AI <- as.numeric(predict(lmvar, newdata=z2))
## Recombine and add together biomass and variances
index <- rbind(z1,z2) %>% group_by(year) %>%
  summarize(biomass=round(biomass_AI+biomass_ebs,5),
            variance=sd_AI^2+sd_ebs^2,
            .groups='drop') %>%
  ## SE on log scale, which SS requires, is sqrt(log(1+CV^2))
  mutate(se_log=round(sqrt(log(1+variance/biomass^2)),5)) %>%
  select(-variance)
SS_index <- data.frame(year=index$year, seas=7, index=2, obs=index$biomass, se_log=index$se_log)
write.csv(x=SS_index, file='inputs/SS_survey_index.csv', row.names=FALSE)

### -----  survey size composition data -----
## Bering sea shelf survey size comp
message("Processing survey length data...")
species <- "10130" #flathead sole only, no BF
lcomp_raw <-
  read.csv('2020/data/lengths_survey_ebs.csv') %>%
  filter(SPECIES_CODE %in% species & STRATUM==999999 & LENGTH>0) %>%
  droplevels() %>% mutate(LENGTH=LENGTH/10) # mm to cm
lcomp <- BIN_LEN_DATA(data=lcomp_raw, len_bins=len_bins) %>%
  select(year=YEAR, males=MALES, females=FEMALES, bin=BIN) %>%
  ## collapse different lengths into bins by year
  group_by(year, bin) %>%
  summarize(males=sum(males), females=sum(females), .groups='drop') %>%
  ## don't arrange by year first or it breaks order of wide
  ## columns below, b/c first year is missing some bins
  arrange(bin, year)
## Calculate annual totals, turn into proportions, and format for SS
lcomp <- group_by(lcomp, year) %>%
  mutate(M_PROP=males/sum(males+females),
         F_PROP=females/sum(males+females)) %>% ungroup() %>%
  select(-males, -females)
lcomp_fem <- select(lcomp, -M_PROP) %>%
  pivot_wider(names_from=bin, names_prefix='f',
              values_from='F_PROP', values_fill=0) %>%
  arrange(year)
lcomp_male <- select(lcomp, -F_PROP) %>%
  pivot_wider(names_from=bin, names_prefix='m',
              values_from='M_PROP', values_fill=0) %>%
  arrange(year)
stopifnot(all.equal(lcomp_fem$year, lcomp_male$year))
## The sample sizes come from the number of hauls with lengths
## which is in this file.
lcomp_Nsamp <- read.csv('data/lengths_survey_hauls.csv')
stopifnot(all(lcomp_Nsamp$YEAR == lcomp_fem$year))
stopifnot(all(!is.na(lcomp_Nsamp$HAULS_W_LENGTH)))
SS_lcomp_survey <- data.frame(year=lcomp_fem$year, month=7,
                              fleet=2, sex=3, part=0, Nsamp=lcomp_Nsamp$HAULS_W_LENGTH,
                              lcomp_fem[,-1], lcomp_male[,-1])
names(SS_lcomp_survey)[1:6] <- names(SS_dat$lencomp)[1:6]
stopifnot(all(1==rowSums(SS_lcomp_survey[,-(1:6)])))
write.csv(SS_lcomp_survey, 'inputs/SS_lcomp_survey.csv', row.names=FALSE)

## ----- survey conditional age at length -----
message("Processing survey age data...")
ebsstandard <- "standard" #use the ebs standard area = "standard", use the plus NW = "plusNW", use GOA = "GOA"
#subareas 1-6 is the standard area, I think
species1 <- 10130 #Flathead sole
species2 <- 10140 #Bering flounder
## 2018 assessment errantly had this as "both" so BF was
## included. Dropped in 2020.
WhichSpecies <- species1 #species1 or species2 or both
AL.df <- read.csv('data/ages_survey_ebs.csv')
source("inputs/process_agecomps_survey.R")
write.csv(SS_caal_survey, file='inputs/SS_caal_survey.csv',
            row.names=FALSE)

### --------------------------------------------------
### I really need to clean up and fix these still. But I got them
### to match. Fix it next time.
## Fishery size comps
gear <- "nonpelagic"
with_unsexed <- FALSE
message("Processing fishery length data...")
source('inputs/process_lcomps_fishery.R')
## And age comps
PullData = 0  # 1 = pull data from SQL; 0 = read it from file
message("Processing fishery age data...")
source('inputs/process_agecomps_fishery.R')
## TO avoid double counting fish we only use lengths when there
## are no age data for that year, set them to a negative fleet
## which tells SS to use them as a ghost fleet but we can still
## see the fits. Some of the ages are ghost fleets so use length
## in those years too
yrs.to.drop <- unique(SS_agecomps_fishery$Yr[SS_agecomps_fishery$FltSvy==1])
SS_lcomps_fishery$FltSvy[SS_lcomps_fishery$Yr %in% yrs.to.drop] <- -1

### --------------------------------------------------
message("Creating new SS data file...")
## Create new SS data file carefully from new inputs. For now I'm
## filtering out > 2018 so I can compare the old assessment with
## the updated data (model 3)
dat <- SS_dat
dat$endyr <- 2020
## The early catches aren't in AKFIN so reuse what was in
## previous assessment. So I'm assuming the old catches are
## reliable and reusing them. Need to confirm with Carey.
dat$catch <- rbind(filter(dat$catch, year<=1995),
                   filter(SS_catch, year>1995))
dat$CPUE <- SS_index
names(SS_lcomps_fishery) <- names(SS_lcomp_survey) <- names(dat$lencomp)
dat$lencomp  <- rbind(SS_lcomp_survey, SS_lcomps_fishery)
names(SS_caal_survey) <- names(SS_agecomps_fishery) <- names(dat$agecomp)
dat$agecomp  <- rbind(SS_caal_survey, SS_agecomps_fishery)
## Check for differences
## saveRDS(dat, 'inputs/datfile.RDS') # update as needed
datold <- readRDS('inputs/datfile.RDS')
stopifnot(all.equal(datold, dat))


### Write a couple of versions of the input file
## The full new data to be used in 2020
SS_writedat(dat, outfile='inputs/2020_BSAI_FHS.dat',
            overwrite=TRUE, verbose=FALSE)
test <- file.copy(from='inputs/2020_BSAI_FHS.dat',
          to='model_runs/Run04_2020_new_data/2020_BSAI_FHS.dat',
          overwrite=TRUE)
if(!test) stop("failed to copy new data file")

## Drop off the new data to compare to previous year as a
## bridging exercise. I want to know if the data mistakes Carey
## made are the cause for any change here, or if it's the new
## data. For bridging to new assessment. Note that new survey ages were
## added for 1999 in 2020 so filter those out.
dat2 <- SS_dat
dat2$endyr <- 2018
dat2$catch <- filter(dat$catch, year<=2018)
dat2$CPUE <- filter(dat$CPUE, year<=2018)
dat2$lencomp <- filter(dat$lencomp, Yr<=2018)
dat2$agecomp <- filter(dat$agecomp, Yr<2018)
## Add 2018 fishery lengths back in since dropped ages
dat2$lencomp[dat2$lencomp$Yr==2018 & dat2$lencomp$FltSvy== -1,'FltSvy']  <- 1
## ## Add 1999 survey lengths back in (why are these turned off??)
## dat2$lencomp[dat2$lencomp$Yr==1999 & dat2$lencomp$FltSvy== 2,'FltSvy']
## Turn off the new survey ages in 1999
dat2$agecomp[dat2$agecomp$Yr==1999 & dat2$agecomp$FltSvy== 2,'FltSvy'] <- -2
SS_writedat(dat2, outfile='inputs/2020_BSAI_FHS_test.dat',
            overwrite=TRUE, verbose=FALSE)
test <- file.copy(from='inputs/2020_BSAI_FHS_test.dat',
          to='model_runs/Run03_2018_new_data/dat_test.dat',
          overwrite=TRUE)
if(!test) stop("failed to copy new data file")

## Data checks comparing the data inputs from last year and this
## year
source("inputs/input_checks.R")

